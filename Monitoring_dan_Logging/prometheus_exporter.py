"""
Prometheus Exporter untuk Model Inference
Menyediakan metrik-metrics untuk monitoring model ML
"""

from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import random
import threading
import logging
from datetime import datetime
import numpy as np

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== METRICS DEFINITION ====================

# 1. Request Metrics
HTTP_REQUESTS_TOTAL = Counter(
    'http_requests_total',
    'Total number of HTTP requests',
    ['method', 'endpoint', 'status']
)

# 2. Inference Metrics
INFERENCE_REQUESTS_TOTAL = Counter(
    'model_inference_requests_total',
    'Total number of inference requests',
    ['model_name', 'model_version']
)

INFERENCE_LATENCY_SECONDS = Histogram(
    'model_inference_latency_seconds',
    'Latency of model inference in seconds',
    ['model_name', 'model_version'],
    buckets=[0.1, 0.25, 0.5, 0.75, 1.0, 2.0, 5.0]
)

# 3. Performance Metrics
MODEL_ACCURACY = Gauge(
    'model_accuracy_current',
    'Current model accuracy (0-1 scale)',
    ['model_name', 'model_version']
)

MODEL_PREDICTIONS_TOTAL = Counter(
    'model_predictions_total',
    'Total number of predictions made',
    ['model_name', 'model_version']
)

MODEL_ERRORS_TOTAL = Counter(
    'model_errors_total',
    'Total number of prediction errors',
    ['model_name', 'model_version', 'error_type']
)

# 4. System Metrics
PREDICTION_CONFIDENCE = Histogram(
    'model_prediction_confidence',
    'Confidence score of predictions',
    ['model_name', 'model_version'],
    buckets=[0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
)

FEATURE_DRIFT = Gauge(
    'model_feature_drift',
    'Feature drift detection score',
    ['model_name', 'feature_name']
)

# 5. Custom Business Metrics
BUSINESS_VALUE = Counter(
    'model_business_value_generated',
    'Estimated business value generated by predictions',
    ['model_name', 'prediction_type']
)

PREDICTION_DISTRIBUTION = Counter(
    'model_prediction_distribution',
    'Distribution of prediction classes',
    ['model_name', 'prediction_class']
)

# ==================== MODEL SIMULATOR ====================

class ModelSimulator:
    """Simulate model predictions and generate metrics"""
    
    def __init__(self, model_name="water_potability_rf", model_version="1.0.0"):
        self.model_name = model_name
        self.model_version = model_version
        self.accuracy = 0.85  # Initial accuracy
        self.running = True
        
    def simulate_request(self):
        """Simulate an HTTP request to the model"""
        start_time = time.time()
        
        # Simulate request processing
        time.sleep(random.uniform(0.05, 0.3))
        
        # Record metrics
        HTTP_REQUESTS_TOTAL.labels(
            method='POST',
            endpoint='/predict',
            status='200'
        ).inc()
        
        latency = time.time() - start_time
        return latency
    
    def simulate_inference(self):
        """Simulate model inference"""
        inference_start = time.time()
        
        try:
            # Simulate inference time
            inference_time = random.uniform(0.1, 0.8)
            time.sleep(inference_time)
            
            # Record inference metrics
            INFERENCE_REQUESTS_TOTAL.labels(
                model_name=self.model_name,
                model_version=self.model_version
            ).inc()
            
            INFERENCE_LATENCY_SECONDS.labels(
                model_name=self.model_name,
                model_version=self.model_version
            ).observe(inference_time)
            
            # Record prediction
            prediction_class = random.choice(['potable', 'not_potable'])
            MODEL_PREDICTIONS_TOTAL.labels(
                model_name=self.model_name,
                model_version=self.model_version
            ).inc()
            
            PREDICTION_DISTRIBUTION.labels(
                model_name=self.model_name,
                prediction_class=prediction_class
            ).inc()
            
            # Record confidence (simulate with some randomness)
            confidence = random.uniform(0.6, 0.98)
            PREDICTION_CONFIDENCE.labels(
                model_name=self.model_name,
                model_version=self.model_version
            ).observe(confidence)
            
            # Simulate business value
            if prediction_class == 'potable':
                business_value = random.uniform(100, 500)
                BUSINESS_VALUE.labels(
                    model_name=self.model_name,
                    prediction_type='potable'
                ).inc(business_value)
            else:
                business_value = random.uniform(50, 200)
                BUSINESS_VALUE.labels(
                    model_name=self.model_name,
                    prediction_type='not_potable'
                ).inc(business_value)
            
            # Occasionally simulate errors (5% error rate)
            if random.random() < 0.05:
                raise ValueError("Simulated prediction error")
                
            return {
                'success': True,
                'inference_time': inference_time,
                'confidence': confidence
            }
            
        except Exception as e:
            # Record error
            MODEL_ERRORS_TOTAL.labels(
                model_name=self.model_name,
                model_version=self.model_version,
                error_type=type(e).__name__
            ).inc()
            return {'success': False, 'error': str(e)}
    
    def update_accuracy(self):
        """Simulate accuracy changes over time (with some drift)"""
        # Simulate gradual accuracy change with some random walk
        change = random.uniform(-0.02, 0.02)
        self.accuracy = max(0.5, min(0.95, self.accuracy + change))
        
        # Update metric
        MODEL_ACCURACY.labels(
            model_name=self.model_name,
            model_version=self.model_version
        ).set(self.accuracy)
        
        logger.debug(f"Updated accuracy: {self.accuracy:.3f}")
        return self.accuracy
    
    def simulate_feature_drift(self):
        """Simulate feature drift detection"""
        features = ['ph', 'hardness', 'solids', 'chloramines', 'sulfate',
                   'conductivity', 'organic_carbon', 'trihalomethanes', 'turbidity']
        
        for feature in features:
            # Simulate drift score (0 = no drift, 1 = high drift)
            drift_score = random.betavariate(2, 8)  # Bias towards low drift
            FEATURE_DRIFT.labels(
                model_name=self.model_name,
                feature_name=feature
            ).set(drift_score)
    
    def run_simulation(self):
        """Run continuous simulation of model activity"""
        logger.info(f"Starting model simulation for {self.model_name} v{self.model_version}")
        
        while self.running:
            try:
                # Simulate request rate (Poisson process)
                requests_this_second = random.poisson(3)  # Avg 3 req/sec
                
                for _ in range(requests_this_second):
                    # Simulate HTTP request
                    http_latency = self.simulate_request()
                    
                    # Simulate inference (90% of requests trigger inference)
                    if random.random() < 0.9:
                        result = self.simulate_inference()
                        
                        # Update accuracy periodically
                        if random.random() < 0.1:  # 10% chance
                            self.update_accuracy()
                
                # Update feature drift less frequently
                if random.random() < 0.01:  # 1% chance per iteration
                    self.simulate_feature_drift()
                
                # Sleep to control rate
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"Error in simulation: {e}")
                time.sleep(5)

# ==================== EXPORTER MAIN ====================

def start_exporter(port=8000):
    """Start the Prometheus metrics exporter"""
    
    # Start HTTP server for Prometheus metrics
    start_http_server(port)
    logger.info(f"Prometheus exporter started on port {port}")
    logger.info(f"Metrics available at http://localhost:{port}/metrics")
    
    # Create and start model simulator
    model_simulator = ModelSimulator()
    
    # Run simulation in background thread
    simulator_thread = threading.Thread(
        target=model_simulator.run_simulation,
        daemon=True
    )
    simulator_thread.start()
    
    # Keep main thread alive
    try:
        while True:
            # Log status periodically
            logger.info("Exporter is running...")
            time.sleep(60)
    except KeyboardInterrupt:
        logger.info("Shutting down exporter...")
        model_simulator.running = False
        simulator_thread.join(timeout=5)

# ==================== METRICS GENERATION ENDPOINT ====================

def generate_sample_metrics():
    """Generate sample metrics for testing"""
    from prometheus_client import generate_latest, REGISTRY
    
    # Simulate some activity
    simulator = ModelSimulator()
    
    # Generate some sample metrics
    for _ in range(10):
        simulator.simulate_request()
        simulator.simulate_inference()
        simulator.update_accuracy()
    
    # Return metrics in Prometheus format
    return generate_latest(REGISTRY)

# ==================== MAIN ENTRY POINT ====================

if __name__ == "__main__":
    import sys
    
    # Parse command line arguments
    port = 8000
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            logger.error(f"Invalid port: {sys.argv[1]}")
            sys.exit(1)
    
    logger.info(f"""
    ============================================
    Model Monitoring Exporter
    ============================================
    Port: {port}
    Model: Water Potability Classifier
    Version: 1.0.0
    Metrics:
      - HTTP Requests: http_requests_total
      - Inference Latency: model_inference_latency_seconds
      - Model Accuracy: model_accuracy_current
      - Total Predictions: model_predictions_total
      - Prediction Confidence: model_prediction_confidence
      - Feature Drift: model_feature_drift
      - Business Value: model_business_value_generated
    ============================================
    """)
    
    # Start the exporter
    start_exporter(port)